{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising Diffusion Probabilistic Models with M&Ms-2 Dataset\n",
    "\n",
    "This tutorial illustrates how to use MONAI for training a denoising diffusion probabilistic model (DDPM)[1] to create\n",
    "synthetic 2D images.\n",
    "\n",
    "[1] - Ho et al. \"Denoising Diffusion Probabilistic Models\" https://arxiv.org/abs/2006.11239\n",
    "\n",
    "\n",
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import torchio\n",
    "import torchmetrics\n",
    "\n",
    "from monai import transforms\n",
    "from monai import handlers\n",
    "from monai.apps import MedNISTDataset\n",
    "from monai.config import print_config\n",
    "from monai.data import CacheDataset, DataLoader\n",
    "from monai.utils import first, set_determinism\n",
    "\n",
    "from generative.inferers import DiffusionInferer\n",
    "from generative.networks.nets import DiffusionModelUNet\n",
    "from generative.networks.schedulers import DDIMScheduler, DDPMScheduler, PNDMScheduler\n",
    "\n",
    "print_config()\n",
    "print(\"TorchIO version:\", torchio.__version__)\n",
    "print(\"TorchMetrics version:\", torchmetrics.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics testing block: IS and FID\n",
    "IS: https://torchmetrics.readthedocs.io/en/stable/image/inception_score.html  \n",
    "FID: https://torchmetrics.readthedocs.io/en/stable/image/frechet_inception_distance.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jrimm\\.conda\\envs\\ddpm\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\jrimm\\.conda\\envs\\ddpm\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(1.0544), tensor(0.0117))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "_ = torch.manual_seed(123)\n",
    "from torchmetrics.image.inception import InceptionScore\n",
    "inception = InceptionScore()\n",
    "# generate some images\n",
    "imgs = torch.randint(0, 255, (100, 3, 299, 299), dtype=torch.uint8)\n",
    "inception.update(imgs)\n",
    "inception.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12.7202)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "_ = torch.manual_seed(123)\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "fid = FrechetInceptionDistance(feature=64)\n",
    "# generate two slightly overlapping image intensity distributions\n",
    "imgs_dist1 = torch.randint(0, 200, (100, 3, 299, 299), dtype=torch.uint8)\n",
    "imgs_dist2 = torch.randint(100, 255, (100, 3, 299, 299), dtype=torch.uint8)\n",
    "fid.update(imgs_dist1, real=True)\n",
    "fid.update(imgs_dist2, real=False)\n",
    "fid.compute()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup data directory\n",
    "\n",
    "You can specify a directory with the MONAI_DATA_DIRECTORY environment variable.\n",
    "\n",
    "This allows you to save results and reuse downloads.\n",
    "\n",
    "If not specified a temporary directory will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set deterministic training for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_determinism(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup M&Ms-2 Dataset and training and validation dataloaders\n",
    "Combine Sina's implementation to import M&Ms-2 dataset (https://www.ub.edu/mnms-2/) manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.dataloader import CMRCineDataModule\n",
    "import argparse\n",
    "import sys\n",
    "sys.argv=['']\n",
    "del sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "PATH = os.path.join(os.getcwd(), \"dataset_3D_crop_small\")\n",
    "RUN_NAME = \"cmr_DDPM_24042023\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print('Device is {}'.format(DEVICE))\n",
    "# print(\"Current device is on\", torch.cuda.current_device())\n",
    "# print(torch.cuda.get_device_name(0))\n",
    "\n",
    "# set your own path here, eg, '/home/bme001/20180883/data/mnms2/sorted/SA/PerDisease' (Linux style path)\n",
    "default_config = {\n",
    "    'dataset_path': PATH,\n",
    "    'run_name': RUN_NAME,\n",
    "    'epochs': 300,\n",
    "    'log_interval': 100,\n",
    "    'batch_size' : 8,\n",
    "    'image_size' : 64,\n",
    "    # TODO: num_workers > 0 causes error on windows\n",
    "    'num_workers' : 0,  # default 8, windows cannot handle this\n",
    "    'device' : DEVICE,\n",
    "    'lr' : 3e-4,\n",
    "    'noise_steps' : 1000,\n",
    "    'beta_start': 1e-4,\n",
    "    'beta_end': 0.02,\n",
    "    }\n",
    "\n",
    "# set key-value pairs from command line \n",
    "parser = argparse.ArgumentParser()\n",
    "for keys in default_config:\n",
    "    parser.add_argument('--'+keys, default=default_config[keys], type= type(default_config[keys]))\n",
    "args = parser.parse_args()\n",
    "\n",
    "# CMR2DDataModule or CMRCineDataModule class\n",
    "data = CMRCineDataModule(\n",
    "        data_dir = args.dataset_path,\n",
    "        image_size = args.image_size,\n",
    "        batch_size = args.batch_size,\n",
    "        train_val_ratio = 0.8,\n",
    "        num_workers = args.num_workers,\n",
    "    )\n",
    "data.prepare_data()\n",
    "data.setup()\n",
    "\n",
    "train_loader = data.train_dataloader()\n",
    "val_loader = data.val_dataloader()\n",
    "\n",
    "print(train_loader.dataset.__len__())\n",
    "print(val_loader.dataset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Batch_size = {}'.format(args.batch_size))\n",
    "print('Number of batches in train_dataloader = {}'.format(len(train_loader)))\n",
    "print('Number of batches in val_dataloader = {}'.format(len(val_loader)))\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "print(batch['image']['data'].squeeze(-1).shape)\n",
    "print(batch['image']['data'].squeeze(-1).max())\n",
    "print(batch['image']['data'].squeeze(-1).min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.utils import plot_batch\n",
    "\n",
    "# TODO: Filter out the useless slices\n",
    "plot_batch(train_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation of the training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_data = first(train_loader)\n",
    "print(f\"batch shape: {check_data['image']['data'].shape}\")\n",
    "# print(check_data[\"image\"]['data'][0, 0].shape)\n",
    "image_visualisation = torch.cat(\n",
    "    [check_data[\"image\"]['data'][0, 0], check_data[\"image\"]['data'][1, 0], \\\n",
    "        check_data[\"image\"]['data'][2, 0], check_data[\"image\"]['data'][3, 0]], dim=1\n",
    ")\n",
    "print(check_data[\"image\"]['data'][0, 0].max())\n",
    "print(check_data[\"image\"]['data'][0, 0].min())\n",
    "plt.figure(\"training images\", (12, 6))\n",
    "plt.imshow(image_visualisation, vmin=-1, vmax=1, cmap=\"gray\")  \n",
    "# plt.imshow(image_visualisation, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define network, scheduler, optimizer, and inferer\n",
    "At this step, we instantiate the MONAI components to create a DDPM, the UNET, the noise scheduler, and the inferer used for training and sampling. We are using\n",
    "the original DDPM scheduler containing 1000 timesteps in its Markov chain, and a 2D UNET with attention mechanisms\n",
    "in the 2nd and 3rd levels, each with 1 attention head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the model\n",
    "device = torch.device(args.device)\n",
    "total_timesteps = args.noise_steps\n",
    "epochs = args.epochs\n",
    "\n",
    "model = DiffusionModelUNet(\n",
    "    spatial_dims = 2,\n",
    "    in_channels = 1,\n",
    "    out_channels = 1,\n",
    "    num_channels = (128, 256, 256),\n",
    "    attention_levels = (False, True, True),\n",
    "    num_res_blocks = 1,\n",
    "    num_head_channels = 256,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "scheduler = DDPMScheduler(num_train_timesteps=total_timesteps)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=args.lr)\n",
    "\n",
    "inferer = DiffusionInferer(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "If you would like to skip the training and use a pre-trained model instead, set `use_pretrained=True`. This model was trained using the code in `tutorials/generative/distributed_training/ddpm_training_ddp.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_pretrained = False  # set to False to train the model from scratch\n",
    "\n",
    "# TODO: Solve the problem CPU bottleneck, eg. CacheDataset https://www.youtube.com/watch?v=0mVgu_DFbeY\n",
    "if use_pretrained:\n",
    "    model = torch.hub.load(\"marksgraham/pretrained_generative_models:v0.2\", model=\"ddpm_2d\", verbose=True).to(device)\n",
    "else:\n",
    "    n_epochs = epochs\n",
    "    val_interval = 30\n",
    "    epoch_loss_list = []\n",
    "    val_epoch_loss_list = []\n",
    "\n",
    "    scaler = GradScaler()\n",
    "    total_start = time.time()\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), ncols=70)\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "        for step, batch in progress_bar:\n",
    "            images = batch[\"image\"]['data'].squeeze(-1).to(device)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            with autocast(enabled=True):\n",
    "                # Generate random noise\n",
    "                noise = torch.randn_like(images).to(device)\n",
    "\n",
    "                # Create timesteps\n",
    "                timesteps = torch.randint(\n",
    "                    0, inferer.scheduler.num_train_timesteps, (images.shape[0],), device=images.device\n",
    "                ).long()\n",
    "\n",
    "                # Get model prediction\n",
    "                noise_pred = inferer(inputs=images, diffusion_model=model, noise=noise, timesteps=timesteps)\n",
    "\n",
    "                loss = F.mse_loss(noise_pred.float(), noise.float())\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            progress_bar.set_postfix({\"loss\": epoch_loss / (step + 1)})\n",
    "        epoch_loss_list.append(epoch_loss / (step + 1))\n",
    "\n",
    "        if (epoch + 1) % val_interval == 0:\n",
    "            model.eval()\n",
    "            val_epoch_loss = 0\n",
    "            for step, batch in enumerate(val_loader):\n",
    "                images = batch[\"image\"]['data'].squeeze(-1).to(device)\n",
    "                with torch.no_grad():\n",
    "                    with autocast(enabled=True):\n",
    "                        noise = torch.randn_like(images).to(device)\n",
    "                        timesteps = torch.randint(\n",
    "                            0, inferer.scheduler.num_train_timesteps, (images.shape[0],), device=images.device\n",
    "                        ).long()\n",
    "                        noise_pred = inferer(inputs=images, diffusion_model=model, noise=noise, timesteps=timesteps)\n",
    "                        val_loss = F.mse_loss(noise_pred.float(), noise.float())\n",
    "\n",
    "                val_epoch_loss += val_loss.item()\n",
    "                progress_bar.set_postfix({\"val_loss\": val_epoch_loss / (step + 1)})\n",
    "            val_epoch_loss_list.append(val_epoch_loss / (step + 1))\n",
    "\n",
    "            # Sampling image during training\n",
    "            noise = torch.randn((1, 1, 64, 64))\n",
    "            noise = noise.to(device)\n",
    "            scheduler.set_timesteps(num_inference_steps=total_timesteps)\n",
    "            with autocast(enabled=True):\n",
    "                image = inferer.sample(input_noise=noise, diffusion_model=model, scheduler=scheduler)\n",
    "\n",
    "            plt.figure(figsize=(2, 2))\n",
    "            plt.imshow(image[0, 0].cpu(), vmin=0, vmax=1, cmap=\"gray\")\n",
    "            plt.tight_layout()\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "\n",
    "    total_time = time.time() - total_start\n",
    "    print(f\"train completed, total time: {total_time}.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.path.join(os.getcwd(), \"checkpoints/checkpoints_T1000_e300\")\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DiffusionModelUNet(\n",
    "    spatial_dims = 2,\n",
    "    in_channels = 1,\n",
    "    out_channels = 1,\n",
    "    num_channels = (128, 256, 256),\n",
    "    attention_levels = (False, True, True),\n",
    "    num_res_blocks = 1,\n",
    "    num_head_channels = 256,\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.path.join(os.getcwd(), \"checkpoints/checkpoints_T1000_e300\")\n",
    "model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_pretrained:\n",
    "    plt.style.use(\"seaborn-v0_8\")\n",
    "    plt.title(\"Learning Curves\", fontsize=20)\n",
    "    plt.plot(np.linspace(1, n_epochs, n_epochs), epoch_loss_list, color=\"C0\", linewidth=2.0, label=\"Train\")\n",
    "    plt.plot(\n",
    "        np.linspace(val_interval, n_epochs, int(n_epochs / val_interval)),\n",
    "        val_epoch_loss_list,\n",
    "        color=\"C1\",\n",
    "        linewidth=2.0,\n",
    "        label=\"Validation\",\n",
    "    )\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.xlabel(\"Epochs\", fontsize=16)\n",
    "    plt.ylabel(\"Loss\", fontsize=16)\n",
    "    plt.legend(prop={\"size\": 14})\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting sampling process along DDPM's Markov chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "noise = torch.randn((1, 1, 64, 64))\n",
    "noise = noise.to(device)\n",
    "scheduler.set_timesteps(num_inference_steps=total_timesteps)\n",
    "with autocast(enabled=True):\n",
    "    image, intermediates = inferer.sample(\n",
    "        input_noise=noise, diffusion_model=model, scheduler=scheduler, save_intermediates=True, intermediate_steps=100\n",
    "    )\n",
    "\n",
    "chain = torch.cat(intermediates, dim=-1)\n",
    "\n",
    "plt.style.use(\"default\")\n",
    "plt.imshow(chain[0, 0].cpu(), vmin=0, vmax=1, cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting multiple generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_generations = 16\n",
    "cols = 4\n",
    "\n",
    "model.eval()\n",
    "noise = torch.randn((n_generations, 1, 64, 64))\n",
    "noise = noise.to(device)\n",
    "scheduler.set_timesteps(num_inference_steps=total_timesteps)\n",
    "with autocast(enabled=True):\n",
    "    image = inferer.sample(\n",
    "        input_noise=noise, diffusion_model=model, scheduler=scheduler\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = image.cpu().squeeze()\n",
    "\n",
    "print(image.shape)\n",
    "# print(image.min())\n",
    "\n",
    "plt.figure()\n",
    "for i in range(n_generations//cols):\n",
    "    for j in range(cols):\n",
    "        plt.subplot(n_generations//cols, cols, i*cols+j+1)\n",
    "        plt.imshow(image[i*cols+j], vmin=0, vmax=1, cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "plt.style.use(\"default\")             \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if directory is None:\n",
    "    shutil.rmtree(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddpm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
